{% extends "base.html" %}
{% block content %}
    <div class="container">
        <h1>Modeling - Naive Bayes</h1>
    </div>
    <br>
    <div class="container">
        <h2>Overview</h2>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            Naive Bayes is a generative supervised machine learning classification method. 
            The generative aspect indicates that the model learns from the probability of the data given previous knowledge of the label. 
            This is in comparison to a discriminative model, where the goal is to find a function which distinguishes between groups (i.e. Logistic Regression). 
            The supervised aspect means that the model is given labels to learn from. 
            The Naïve aspect comes from the assumption that the categories have conditional independence in order to apply the Bayes’ Theorem. 
            This is a Naive assumption because it’s unlikely that the variables within the data have true independence. 
            For example, consider a model built from customer reviews which is trying to classify if the review was positive or negative. 
            The review might have language such as “happy” and “glad”, which are clearly not independent terms. 
            However, the assumption of independence is made to allow for the calculations to work. 
            This example is one of sentiment analysis, however there are many potential applications of this method as it is implicitly acceptable for n-class classification. 
            For example, predicting weather a label is “true” or “false” is a 2-class problem, but Naive Bayes can extend this to multiple labels. 
            Back to the sentiment analysis example, a review could be “positive”, “negative”, or “neutral”. 
            Applications also include document classification, which could be used to classify an article into categories such as “politics”, “sports”, “entertainment”, among many other overall article types.
        </p>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            In general, Naive Bayes uses the conditional independence assumption to apply the Bayes’ Theorem. 
            Essentially, the goal is to find the probability of a label given a datapoint. 
            The Bayes’ Theorem is appropriate for this task and uses several components of the probabilities within the data to calculate this. 
            Namely, the probability of the data itself occurring, the probability of the label occurring, and the conditional probability in the opposite direction (i.e. the probability of the datapoint given a label). 
            Especially in larger datasets, some of the conditional probabilities can be zero. 
            This presents an issue due to the multiplicative calculations required, which would zero out the entire probability. 
            Smoothing techniques are used to account for this, with the Laplacian Correction being the most common. 
            This technique adds 1 to each case’s count. 
            The general smoothing technique adds a specified variable (or alpha) to the count.
        </p>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            There are several primary forms of Naive Bayes:
            <br>
            <ul>
                <li>Multinomial Naive Bayes</li>
                <li>Gaussian Naive Bayes</li>
                <li>Bernoulli Naive Bayes</li>
                <li>Categorical Naive Bayes</li>
            </ul>
        </p>
        <br>
        
        <h3>Multinomial Naive Bayes</h3>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            Multinomial Naive Bayes "is suitable for classification within discrete features (e.g., word counts for text classification)."<a href='https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html'>[sklearn documentation]</a>. 
            Other discrete feature applications could be spam detection, sentiment analysis, and document categorization. 
            Overall, this is a very efficient naive bayes method for text-based tasks.
        </p>
        <br>
        
        <h3>Gaussian Naive Bayes</h3>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            Gaussian Naive Bayes can be used when features are continuous, and are assumed to follow a normal (or Gaussian) distribution. 
            Some of the specific applications could be medical diagnoses, real-time classification, and anomaly detection.
            
        </p>
        <br>
        
        <h3>Bernoulli Naive Bayes</h3>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            Bernoulli Naive Bayes is applicable for discrete data, specifically when features are binary. 
            To extend from the multinomial example, instead of word counts, the data should be either 0s or 1s for if the word was included. 
            The other applications from the multinomial applications would be eligible as well, as long as it is used in a binary sense.
        </p>
        <br>
        
        <h3>Categorical Naive Bayes</h3>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            Categorical Naive Bayes is "suitable for classification with discrete features that are categorically distributed" <a href='https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html'>[sklearn documentation]</a>. 
            Note that this method requires encoding which represents the categorical variables as numerical. Ordinal encoding ensures order of the categorical variables (i.e. letter grades have an order), 
            and non-ordinal categorical data (i.e. no implied order) can still be encoded. 
            Some specific applications could be retail and marketing customer segmentation, healthcare classification, and others. Essentially, if categorical columns are present in the classification problem, this could be a decent method.
        </p>
        <br>
        
        <h3>Analysis Code</h3>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            The code for this analysis can be found <a href='https://github.com/clickityKlein/snowbound/blob/main/snowbound/scripts/modeling/nb/naive_bayes_coding.py'>[here]</a>.
        </p>
        <br>
        
    </div>
    <div class="container">
        <h2>Data Preparation</h2>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            This project will be applying three of the Naive Bayes techniques:
            <br>
            <ul>
                <li>Multinomial Naive Bayes</li>
                <li>Gaussian Naive Bayes</li>
                <li>Bernoulli Naive Bayes</li>
            </ul>
            <br>
            Each of the methods requires different data preparation, which will be detailed below.
        </p>
        <br>
        <h3>Initial Data</h3>
        <p style="font-size: 20px; text-align: justify;">
            Following from the previous sections, this section will use the same datasets. As a reminder, here is a preview of the data that will be used:
        </p>
        <br><br>
        
        <h5>Ski Resorts Data</h5>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ resorts_final | safe }}
        </div>
        <br><br>
        
        <h5>Weather Data</h5>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ weather_final | safe }}
        </div>
        <br><br>
        
        <h5>Google Places Data</h5>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ weather_final | safe }}
        </div>
        <br><br>
        
        <p style="font-size: 20px; text-align: justify;">
            Using these datasets, the following preparation will take place to prepare them for the different Naive Bayes methods.
        </p>
        <br><br>
        
        <h4>Multinomial Naive Bayes</h4>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            The most recent full year of data for weather data was from 2023, and the weather type occurences are currently in binary format for daily data. 
            This data was summed for each resort for the 2023 year.
            <br><br>
            The ski resort data additionally has the number of trails per difficulty and the number lifts, which will be used in this analysis.
            <br><br>
            Using this count data, the type of Pass for resorts will attempted to be modeled. The possible options are:
            
            <ul>
                <li>Ikon</li>
                <li>Epic</li>
                <li>Other</li>
            </ul>
            
            Below is a snippet of the prepared data for the Multinomial Naive Bayes data.
        </p>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ multinomial_snippet | safe }}
        </div>
        <br>
        
        <h4>Gaussian Naive Bayes</h4>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            Due to the machine learning method being able to continuous data, the entirety of the weather dataset was used for this Naive Bayes technique.
            <br><br>
            However, a small change was made to the label data for this dataset. To balance the label dataset, "partly-cloudy-day", "cloudy", "wind", and "fog" were transformed into "other".
            <br><br>
            This reduced the overall labels into:
            
            <ul>
                <li>snow</li>
                <li>rain</li>
                <li>clear-day</li>
                <li>Other</li>
            </ul>
            
            Below is a snippet of the prepared data for the Gaussian Naive Bayes data.
        </p>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ gaussian_snippet | safe }}
        </div>
        <br>
        
        <h4>Bernoulli Naive Bayes</h4>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            For the Bernoulli setup, the data used was to test the efficacy of the Google Places API itself. The data was encoded to account for the return category calls for the API in 0s and 1s.
            <br><br>
            Using this binomial type data, the following label categories are:
            
            <ul>
                <li>Medical</li>
                <li>Lodging</li>
                <li>Shopping</li>
                <li>Restaurants</li>
                <li>Grocery</li>
                <li>Spas</li>
                <li>Bars</li>
            </ul>
            
            <br><br>
            Essentially, the Google API also returns a list of accompanying categories for the call category. This will test if there are any patterns between the call category and the other returns.
            
            <br><br>
            
            Below is a snippet of the prepared data for the Multinomial Naive Bayes data.
        </p>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ bernoulli_snippet | safe }}
        </div>
        <br>
        <h3>Training and Testing Sets</h3>
        <p style="font-size: 20px; text-align: justify;">
            Additionally, training and testing sets were created. The two sets are disjoint, and must be disjoint. 
            Using non-disjoint data between testing and training won't give an accurate representation of the performance of the model. 
            First, this could result in an overfit of the model, which could end up describing noise, rather than the underlying distribution. 
            Second, the testing set being non-disjoint helps to represent real-world data (i.e. unseen data). 
        </p>
        <br>
        
        <h5>Multinomial Naive Bayes Training Dataset</h5>
        <br>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ multinomial_train_snippet | safe }}
        </div>
        <br>
        
        <h5>Multinomial Naive Bayes Testing Dataset</h5>
        <br>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ multinomial_test_snippet | safe }}
        </div>
        <br>
        
        <h5>Gaussian Naive Bayes Training Dataset</h5>
        <br>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ gaussian_train_snippet | safe }}
        </div>
        <br>
        
        <h5>Gaussian Naive Bayes Testing Dataset</h5>
        <br>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ gaussian_test_snippet | safe }}
        </div>
        <br>
        
        <h5>Bernoulli Naive Bayes Training Dataset</h5>
        <br>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ bernoulli_train_snippet | safe }}
        </div>
        <br>
        
        <h5>Bernoulli Naive Bayes Testing Dataset</h5>
        <br>
        <div class="table-responsive" style="max-height: 500px; overflow-y: auto;">
            {{ bernoulli_test_snippet | safe }}
        </div>
        <br>
    </div>
    <br>
    
    <div class="container">
        <h2>Code</h2>
        <p style="font-size: 20px; text-align: justify;">
            Once again, the code can be found <a href='https://github.com/clickityKlein/snowbound/blob/main/snowbound/scripts/modeling/nb/naive_bayes_coding.py'>[here]</a>.
        </p>
    </div>
    <br>
    
    <div class="container">
        <h2>Results</h2>
        <p style="font-size: 20px; text-align: justify;">
            For each method, the balance of the labels was examined, the accuracy of the modeling on the test set was reported, and the confusion matrix was created.
        </p>
        <br><br>
        <h3>Multinomial Naive Bayes</h3>
        <br>
        <figure>
            <img src="{{ url_for('static', filename='models/nb/balance_Multinomial.png') }}" width="100%">
                <figcaption style="text-align: center;">
                    Balance Across Labels for Multinomial Naive Bayes.
                </figcaption>
        </figure>
        <br>
        
        <p style="font-size: 20px; text-align: justify;">
            The accuracy for this result was 75.44%.
        </p>
        <br>
        
        <figure>
            <img src="{{ url_for('static', filename='models/nb/cm_Multinomial.png') }}" width="100%">
                <figcaption style="text-align: center;">
                    Confusion Matrix for Multinomial Naive Bayes.
                </figcaption>
        </figure>
        <br>
        
        <p style="font-size: 20px; text-align: justify;">
            Although the accuracy would be acceptable in many applications, the balance of the labels were significantly skewed towards the Other type pass. 
            Given that the model was trained on such a heavily unbalanced dataset, the accuracy is slightly misleading. Looking at the confusion matrix, 
            Epic pass didn't get a single correct prediction, and Ikon had more total incorrect predictions than correct.
            <br><br>
            
            Balancing the dataset or experimenting with hyperparameters could be beneficial for this problem.
        </p>
        <br>
        
        <h3>Gaussian Naive Bayes</h3>
        <br>
        <figure>
            <img src="{{ url_for('static', filename='models/nb/balance_Gaussian.png') }}" width="100%">
                <figcaption style="text-align: center;">
                    Balance Across Labels for Gaussian Naive Bayes.
                </figcaption>
        </figure>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            The accuracy for this result was 68.25%.
        </p>
        <br>
        
        <figure>
            <img src="{{ url_for('static', filename='models/nb/cm_Gaussian.png') }}" width="100%">
                <figcaption style="text-align: center;">
                    Confusion Matrix for Gaussian Naive Bayes.
                </figcaption>
        </figure>
        <br>
        
        <p style="font-size: 20px; text-align: justify;">
            Balancing was attempted in this problem by combining the low occcurence labels into an Other category. 
            However, the labels weren't perfectly balanced. This was a rather large dataset, so there should be plenty of data to train the labels of lesser proportion.
            Weather prediction is a notoriously difficult problem, and some possible improvements to this specific data could be to include location data, at least at a regional scale. Or, 
            perform some hyperparameter tuning.
        </p>
        <br>
        
        <h3>Bernoulli Naive Bayes</h3>
        <br>
        <figure>
            <img src="{{ url_for('static', filename='models/nb/balance_Bernoulli.png') }}" width="100%">
                <figcaption style="text-align: center;">
                    Balance Across Labels for Bernoulli Naive Bayes.
                </figcaption>
        </figure>
        
        <br>
        <p style="font-size: 20px; text-align: justify;">
            The accuracy for this result was 92.94%.
        </p>
        <br>
        
        <figure>
            <img src="{{ url_for('static', filename='models/nb/cm_Bernoulli.png') }}" width="100%">
                <figcaption style="text-align: center;">
                    Confusion Matrix for Bernoullie Naive Bayes.
                </figcaption>
        </figure>
        <br>
        
        <p style="font-size: 20px; text-align: justify;">
            Given that the accuracy shows a successful model, this does corroborate a pattern between the sub-categories and the parent call category given to the API. 
            Hyperparameter tuning could result in better performance here, however, the incorrect predictions could actually present some correlations worth investigating. 
            For instance, the label of Bars was correctly predicted more than it was not. However, the incorrect predictions for that label was most prevalant was Restaurants. 
            Bars and Restaurants are often correlated together, and more correlations like this do appear in this confusion matrix.
        </p>
        <br>
        
    </div>
    <br>
    
    <div class="container">
        <h2>Conclusions</h2>
        <br>
        <p style="font-size: 20px; text-align: justify;">
            In this analysis, several different patterns were explored.
            <br><br>
            Potential links between daily weather events and the types of ski resort passes was invesigated. This didn't yield great results, and could require more data or 
            different methodology for a future analysis.
            <br><br>
            The relationship between real-time weather outcomes and the general weather descriptions for each day was examined. The results for this did yield more promising results, although 
            location based data or further refinement of the methods might yield better correlations.
            <br><br>
            The relationships between different categories of businesses located near ski resorts was assessed. If someone were to search for a specific service on Google for businesses near 
            ski resorts, they would likely receive the services that were requested. The findings also suggest that if the services wasn't exactly what they were requested, it would be highly correlated. 
            For example, If a user searched for a bar, they receive results for a restaurant.
        </p>
    </div>
    
    <br>
    <br>
{% endblock %}